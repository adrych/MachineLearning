{
 "metadata": {
  "name": "",
  "signature": "sha256:9ce4bae8d2f3ee0e5138ee4540f8d0ad8ae1961fad08d11f057b9907df74038b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Intermezzo: Sci-kit learn\n",
      "\n",
      "## Uczenie za pomoc\u0105 gotowych pakiet\u00f3w\n",
      "\n",
      "B\u0119dziemy dzi\u015b korzysta\u0107 z zaawansowanego pakietu do sieci neuronowych \"Keras\". \n",
      "\n",
      "### Instalacja\n",
      "* Pod Windowsem powinna by\u0107 zainstalowana wersja 0.16.\n",
      "* Pod Linuxem powinna by\u0107 zainstalowana wersja 0.15.\n",
      "* Najnowsza oficjalna wersja to 0.17 (0.18 na githubie w wersji deweloperskiej)\n",
      "\n",
      "* Pod Linuxem mo\u017cna zainstalowa\u0107 lokalnie:\n",
      "        pip install --user sklearn\n",
      "* Lub na w\u0142asnym komputerze:\n",
      "        sudo pip install sklearn\n",
      "* Mo\u017cliwe, \u017ce konieczne jest ponowne uruchomienie IPython, je\u015bli by\u0142 uruchomiony podczas instalacji.\n",
      "\n",
      "### Dokumentacja\n",
      "* Pe\u0142na i obszerna dokumentacja na: http://scikit-learn.org/0.15 (wystarczy zmieni\u0107 numer wersji zgodnie z zainstalowan\u0105 wersj\u0105, aby przej\u015b\u0107\u00a0do dokument\u00f3w dla odpowiedniej wersji)\n",
      "\n",
      "### Zadania teoretyczne\n",
      "* Zapoznaj si\u0119\u00a0z pakietem Sci-Kit learn na podstawie dokumentacji. \n",
      "* Przegl\u0105daj\u0105c spis API (http://scikit-learn.org/0.15/modules/classes.html), podaj list\u0119 dost\u0119pnych rodzaj\u00f3w klasyfikator\u00f3w i regresor\u00f3w. Kt\u00f3re metody poznali\u015bmy na wyk\u0142adach? Kt\u00f3re s\u0105 nowe?\n",
      "* Jakie inne tematy widzisz (ni\u017c\u00a0klasyifikacja/regresja), kt\u00f3re by\u0142y omawiane na wyk\u0142adach?\n",
      "\n",
      "### Zadanie praktyczne\n",
      "* Opracuj tutorial http://scikit-learn.org/0.15/tutorial/text_analytics/working_with_text_data.html w IPython Notebook (do wykonania w tym tygodniu do najbli\u017cszych \u0107wicze\u0144) \n",
      "\n",
      "Powy\u017csze zadania s\u0105 punktowane razem na 40 punkt\u00f3w.\n",
      "\n",
      "### Zadania na te i nast\u0119pne \u0107wiczenia\n",
      "* Opracuj zadanie Exercise 1 lub Exercise 2 (do wyboru) na za dwa tygodnie w postaci IPython Notebook. \n",
      "* B\u0119dzie mo\u017cna te\u017c pracowa\u0107 przy tym na najbliszych \u0107wiczeniach.\n",
      "* Wymagania:\n",
      "    * Dane do tych zada\u0144 trzeba zdoby\u0107 samodzielnie. Dopuszczam dzielenie si\u0119\u00a0danymi, je\u015bli kto\u015b ju\u017c\u00a0co\u015b\u00a0opracowa\u0142/znalaz\u0142.\n",
      "    * Wybierz i sprawd\u017a co najmiej cztery r\u00f3\u017cne rodzaje klasyfikator\u00f3w/regresor\u00f3w, z tego co najmniej dwa, kt\u00f3ry nie by\u0142y dot\u0105d omawiane na \u0107wiczeniach (np. SVM, Naiwny Bayes) lub wyk\u0142adach (drzewa decyzyjne).\n",
      "    * Zawsze wykonaj walidacje krzy\u017cow\u0105 (5-krotn\u0105) i przedstaw wyniki. \n",
      "    * Dodatkowe: Spr\u00f3buj wykona\u0107 optymalizacj\u0119 hiperparamtr\u00f3w za pomoc\u0105 GridSearch lub RandomSearch. Pomog\u0142o?\n",
      "\n",
      "To zadanie b\u0119dzie warte 40 punkt\u00f3w z nast\u0119pnych \u0107wicze\u0144 + 20 punkt\u00f3w za szczeg\u00f3lnie \u0142adne opracowania.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Zadania teoretyczne:\n",
      "Klasyfikatory i regresory: Generalized Linear Methods, Multiclass and multilabel classification, Naive Bayes, Cross decomposition, Pipeline, Gaussian Processess, Isotonic Regression, Kernel Ridge Regression, Discriminant Analysis, Gaussian Mixture models, Nearest Neighbours, Semi-Supervised Learning, Decision Trees, Neural network models, SVM\n",
      "\n",
      "Inne tematy: Adaboost, Bagging, Boosting, Cross Validation, Ensemble Methods\n",
      "\n",
      "Omawiali\u015bmy: Adaboost, Bagging, Boosting, Cross Validation, Ensemble Methods, Generalized Linear Methods, Multiclass and multilabel classification, Naive Bayes, Cross decomposition, Nearest Neighbours, Semi-Supervised Learning, Decision Trees, Neural network models, SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Opracowanie tutoriala\n",
      "\n",
      "#\u015bci\u0105gamy zestaw danych\n",
      "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sprawdzamy jakie mamy nazwy kategorii poprzez target_names\n",
      "twenty_train.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sprawdzamy ile mamy za\u0142adowanych plik\u00f3w (znajduj\u0105 si\u0119 w \"data\")\n",
      "print len(twenty_train.data)\n",
      "#ich nazwy znajduj\u0105 si\u0119 w \"filenames\"\n",
      "print len(twenty_train.filenames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2257\n",
        "2257\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sprawdzamy 3 pierwsze dane\n",
      "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
      "print\n",
      "print(twenty_train.target_names[twenty_train.target[0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: sd345@city.ac.uk (Michael Collier)\n",
        "Subject: Converting images to HP LaserJet III?\n",
        "Nntp-Posting-Host: hampton\n",
        "\n",
        "comp.graphics\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#atrybuty s\u0105 przechowywane jako integers\n",
      "twenty_train.target[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ale mo\u017cemy odzyska\u0107 ich nazwy przez odno\u015bniki do tych liczbb\n",
      "for t in twenty_train.target[:10]:\n",
      "    print(twenty_train.target_names[t])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "comp.graphics\n",
        "comp.graphics\n",
        "soc.religion.christian\n",
        "soc.religion.christian\n",
        "soc.religion.christian\n",
        "soc.religion.christian\n",
        "soc.religion.christian\n",
        "sci.med\n",
        "sci.med\n",
        "sci.med\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bags of words - aby u\u017cyc danych w klasyfikatorach musimy nazwy przekonwertowa\u0107 na liczby\n",
      "# dlatego ka\u017cdemu s\u0142owu przypisujemy liczb\u0119\n",
      "# dla ka\u017cdego dokuemtnu #i, liczymy liczb\u0119 wyst\u0105pie\u0144 ka\u017cdego s\u0142owa w i zapisujemy w X[i, j] \n",
      "# jako warto\u015b\u0107 cechy #j gdzie j jest indeksem s\u0142owa w w s\u0142owniku\n",
      "# ze wzgl\u0119du na du\u017c\u0105 liczb\u0119 danych przechowujemy jedynie niezerowe warto\u015bci\n",
      "\n",
      "#text preprocessing, tokenizing, filtering - budujemy s\u0142ownik cech i przekszta\u0142camy dokumenty w wektory tych cech\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vect = CountVectorizer()\n",
      "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
      "X_train_counts.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "(2257, 35788)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#vectorizer posiada wbudowany s\u0142ownik indeks\u00f3w cech\n",
      "#mo\u017cemy u\u017cy\u0107 indeksu ka\u017cdego s\u0142owa w s\u0142owniku aby uzyska\u0107 cz\u0119stotliwo\u015b\u0107 wyst\u0105pie\u0144\n",
      "count_vect.vocabulary_.get(u'algorithm')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "4690"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#stosujemy Term Frequencing aby obni\u017cy\u0107 warto\u015bci liczb wyst\u0105pie\u0144 cech poprzez dzielenie ich przez sum\u0119 wszystkich s\u0142\u00f3w\n",
      "#dodatkowo dla ka\u017cdej cechy przypisujemy pewn\u0105 wag\u0119, dzi\u0119ki czemu cz\u0119sto wyst\u0119puj\u0105ce s\u0142owa maj\u0105 mniejsze wagi, bo nie maj\u0105\n",
      "#takie znaczenia, poniewa\u017c je\u015bli s\u0142owo jest wsz\u0119dzie, to nie wskazuje na nic konkretnego\n",
      "#t\u0105 metod\u0119 nazywamy Term Frequency times Inverse Document Frequency\n",
      "#obie metody - tf i tf -idf mamy w scikit\n",
      "\n",
      "#najpierw u\u017cywamy metody \"fit\" aby dopasowa\u0107 nasz estymator do danych\n",
      "#potem u\u017cywamy metody \"transform\" kt\u00f3ra wykonuje zadanie\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
      "X_train_tf = tf_transformer.transform(X_train_counts)\n",
      "X_train_tf.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(2257, 35788)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#obie metody mo\u017cna po\u0142\u0105czy\u0107 \u017ceby zmniejszy\u0107 liczb\u0119 powtarzanych oblicze\u0144\n",
      "tfidf_transformer = TfidfTransformer()\n",
      "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
      "X_train_tfidf.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(2257, 35788)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#w ten spos\u00f3b mamy ju\u017c nasze cechy!\n",
      "#teraz przyda\u0142by si\u0119 klasyfikator\n",
      "#na potrzeby tego tutoriala u\u017cywamy Naive Bayes, w wariancie Multinomial\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cechy wyci\u0105gamy podobnie jak wcze\u015bniej, ale bez funkcji \"fit\" bo nasze dane s\u0105 ju\u017c dopasowane\n",
      "\n",
      "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
      "X_new_counts = count_vect.transform(docs_new)\n",
      "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
      "\n",
      "predicted = clf.predict(X_new_tfidf)\n",
      "\n",
      "for doc, category in zip(docs_new, predicted):\n",
      "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'God is love' => soc.religion.christian\n",
        "'OpenGL on the GPU is fast' => comp.graphics\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#aby \u0142atwiej nam si\u0119 pracowa\u0142o z sekwencj\u0105 vectorizer => transformer => classifier\n",
      "#scikit udost\u0119pnia nam klas\u0119 Pipeline kt\u00f3ra zachowuje si\u0119 jak z\u0142o\u017cony klasyfikator:\n",
      "from sklearn.pipeline import Pipeline\n",
      "text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', MultinomialNB()),])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#teraz mo\u017cemy wytrenowa\u0107 nasz model\n",
      "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#oszacowanie accuracy jest w scikit bardzo proste:\n",
      "#klasyfikujemy dane i por\u00f3wnujemy do prawid\u0142owych odpowiedzi, obliczamy \u015bredni\u0105\n",
      "import numpy as np\n",
      "twenty_test = fetch_20newsgroups(subset='test',\n",
      "    categories=categories, shuffle=True, random_state=42)\n",
      "docs_test = twenty_test.data\n",
      "predicted = text_clf.predict(docs_test)\n",
      "np.mean(predicted == twenty_test.target) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "0.83488681757656458"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nasze accuracy wynosi 0,83. Dla por\u00f3wnania sprawdzamy z klasyfiatorem SVM (support vector machine)\n",
      "#warto wspomnie\u0107, \u017ce SVM jest wolniejszy, ale jest uwa\u017cany za najlepszy klasyfikator tekstu\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
      "                                           alpha=1e-3, n_iter=5)),])\n",
      "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
      "predicted = text_clf.predict(docs_test)\n",
      "np.mean(predicted == twenty_test.target)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.90812250332889477"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#wystarczy\u0142o podmieni\u0107 klasyfikator w klasie Pipeline\n",
      "#widzimy \u017ce rzeczywi\u015bcie wynik jest o wiele lepszy! (0.91)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#je\u017celi jeste\u015bmy ciekawi szczeg\u00f3\u0142\u00f3w klasyfikacji mo\u017cemy u\u017cy\u0107 metrics, aby je wy\u015bwietli\u0107\n",
      "#mo\u017cemy sprawdzy\u0107 wynik dla ka\u017cdej klasy osobno\n",
      "from sklearn import metrics\n",
      "print(metrics.classification_report(twenty_test.target, predicted,\n",
      "    target_names=twenty_test.target_names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                        precision    recall  f1-score   support\n",
        "\n",
        "           alt.atheism       0.94      0.82      0.87       319\n",
        "         comp.graphics       0.86      0.98      0.92       389\n",
        "               sci.med       0.95      0.88      0.91       396\n",
        "soc.religion.christian       0.90      0.94      0.92       398\n",
        "\n",
        "           avg / total       0.91      0.91      0.91      1502\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#czego nam brakuje, to dodatkowe parametry do sprawdzania naszych klasyfikator\u00f3w\n",
      "#naturalnie scikit oferuje nam takie algortmy jak SGD z parametrem alpha\n",
      "#mo\u017cemy te parametry ustawi\u0107, ale mamy tu co\u015b lepszego! mo\u017cemy uruchomi\u0107 algorytm GridSearch, kt\u00f3ry znajdzie dla nas najlepsze:\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__alpha': (1e-2, 1e-3),}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#poniewa\u017c takie szukanie mo\u017ce by\u0107 bardzo procesoro-ch\u0142onne, mo\u017cemy ustawi\u0107 parametr n_job na -1, przez co metoda zostanie\n",
      "#dopasowana do mozliwo\u015bci naszego komputera\n",
      "\n",
      "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#testujemy na mniejszej ilo\u015bci danych\n",
      "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#w rezultacie otrzymujemy klasyfikator. Sprawd\u017amy wi\u0119c..\n",
      "twenty_train.target_names[gs_clf.predict(['God is love'])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "'soc.religion.christian'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sprawdzamy grid_scores otrzymanego obiektu, kt\u00f3ry to atrybut jest list\u0105 par parametr-wynik\n",
      "#sprawd\u017amy wi\u0119c parametry z najlepszymi wynikami\n",
      "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "0.89749999999999996"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#widzimy wi\u0119c jakie parametry wybra\u0142 nam algorytm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#poni\u017cej Exercise 2 z Naive Bayes, SGD, Decision Trees, kNN, Neural Network (multi-layer perceptron (MLP) algorithm that trains using backpropagation)\n",
      "%matplotlib inline\n",
      "import sys\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.datasets import load_files\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "from sklearn import cross_validation\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "limit = 400 #limit danych\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Exercise 2  Naive Bayes\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
      "    # block to be able to use a multi-core grid search that also works under\n",
      "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
      "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
      "    # that is used when n_jobs != 1 in GridSearchCV\n",
      "\n",
      "    # the training data folder must be passed as first argument\n",
      "    movie_reviews_data_folder = './movies_opinions/'\n",
      "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
      "    print(\"n_samples: %d\" % len(dataset.data))\n",
      "\n",
      "    # split the dataset in training and test set:\n",
      "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
      "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
      "\n",
      "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
      "    # that are too rare or too frequent\n",
      "     \n",
      "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', MultinomialNB()),])\n",
      "    text_clf = text_clf.fit(docs_train, y_train)\n",
      "\n",
      "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
      "    # more useful.\n",
      "    # Fit the pipeline on the training set using grid search for the parameters\n",
      "    \n",
      "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__alpha': (1e-2, 1e-3),}\n",
      "    \n",
      "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
      "    \n",
      "    gs_clf = gs_clf.fit(docs_train, y_train)\n",
      "    \n",
      "    params = gs_clf.grid_scores_\n",
      "    \n",
      "\n",
      "    # TASK: print the cross-validated scores for the each parameters set\n",
      "    # explored by the grid search\n",
      "    \n",
      "    print \"Parameters sets with scores:\"\n",
      "    i = 0\n",
      "    for p in params:\n",
      "        i+=1\n",
      "        print \"Parameters set\" , i\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"%s: %r\" % (param_name, p[0][param_name]))\n",
      "        print \"Score: \" , p[1]\n",
      "        print\n",
      "\n",
      "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
      "    # named y_predicted\n",
      "    \n",
      "    y_predicted = gs_clf.predict(docs_test)\n",
      "\n",
      "    \n",
      "    # Print the classification report\n",
      "    print(metrics.classification_report(y_test, y_predicted,\n",
      "                                        target_names=dataset.target_names))\n",
      "\n",
      "    # Print and plot the confusion matrix\n",
      "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
      "    print(cm)\n",
      "\n",
      "\n",
      "    plt.matshow(cm)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n_samples: 2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parameters sets with scores:\n",
        "Parameters set 1\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.740666666667\n",
        "\n",
        "Parameters set 2\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.788666666667\n",
        "\n",
        "Parameters set 3\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.781333333333\n",
        "\n",
        "Parameters set 4\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.800666666667\n",
        "\n",
        "Parameters set 5\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.709333333333\n",
        "\n",
        "Parameters set 6\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.765333333333\n",
        "\n",
        "Parameters set 7\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.756\n",
        "\n",
        "Parameters set 8\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.791333333333\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.88      0.83      0.85       257\n",
        "        pos       0.83      0.88      0.85       243\n",
        "\n",
        "avg / total       0.85      0.85      0.85       500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[213  44]\n",
        " [ 30 213]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABWNJREFUeJzt2zGLZHUaxeHz7gx+CRnoREE33okn7MxQJjTW3HCY7+Bu\nthrqpiZiNotgsGsmroIGDaOBkR9A4b/BtNDC1FQxVd239+zzQEHfquJyoPlxb3dXz1orQJc/bT0A\nOD1hQyFhQyFhQyFhQyFhQyFhv8DMnM/MdzPz/cy8v/UeDjMzH87MzzPz9dZbtiLsHWbmTpIPkpwn\neTPJw5l5Y9tVHOijPPu+/d8S9m73k/yw1rpYa/2a5JMkb228iQOstb5I8svWO7Yk7N1eTfL0yvGP\nl8/BrSfs3XzWlv9Zwt7tpyT3rhzfy7OrNtx6wt7tqySvzczZzLyS5O0kn268CQ4i7B3WWr8leS/J\n50n+k+Qfa61vt13FIWbm4yRfJnl9Zp7OzDtbb7pp4982oY8rNhQSNhQSNhQSNhQSNhS6e+wJZsav\n1WEja6153vNHh50kj05xklvqSZIHG2+4To/zzdYTrtlfk7y79Yhr8uedr7gVh0LChkLC3uNs6wEc\n6S9bD9iEsPc423oAR7q/9YBNCBsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsK\nCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsK\nCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsK7Q17Zs5n5ruZ+X5m3r+JUcBx\nXhj2zNxJ8kGS8yRvJnk4M2/cxDDg5e27Yt9P8sNa62Kt9WuST5K8df2zgGPsC/vVJE+vHP94+Rxw\ni93d8/o65CRPrnx9dvkATu1fSf590Dv3hf1TkntXju/l2VX7Dx4cOAs4xv3Lx+/+tvOd+27Fv0ry\n2syczcwrSd5O8unR+4Br9cIr9lrrt5l5L8nnSe4k+fta69sbWQa8tH234llrfZbksxvYApyIT55B\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFD\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFD\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDoVlrHXeCmZU8OtEcbtqjPN56Ai/p\ncZK11jzvNVdsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRs\nKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRs\nKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKLQ37Jn5cGZ+npmv\nb2IQcLxDrtgfJTm/7iHA6ewNe631RZJfbmALcCJ+xoZCd09zmidXvj67fACndHH5OMSJwn5wmtMA\nO53lj5fMf77gvW7FodAhf+76OMmXSV6fmacz8871zwKOsfdWfK318CaGAKfjVhwKCRsKCRsKCRsK\nCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsK\nCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsKCRsK\nCRsKCRsKCRsKCRsKCRsKCXuvi60HcISLrQdsRNh7XWw9gCNcbD1gI8KGQsKGQrPWOu4EM8edAHhp\na6153vNHhw3cPm7FoZCwoZCwoZCwoZCwodB/ASjXipW5PKJwAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fe320c5a410>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Exercise 2  SGD\n",
      "\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    movie_reviews_data_folder = './movies_opinions/'\n",
      "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
      "    print(\"n_samples: %d\" % len(dataset.data))\n",
      "\n",
      "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
      "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
      "\n",
      "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)),])\n",
      "    text_clf = text_clf.fit(docs_train[:limit], y_train[:limit])\n",
      "\n",
      "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__alpha': (1e-2, 1e-3), \n",
      "              }\n",
      "    \n",
      "    \n",
      "    print \"grid search\"\n",
      "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
      "    gs_clf = gs_clf.fit(docs_train[:limit], y_train[:limit])    \n",
      "    params = gs_clf.grid_scores_\n",
      "\n",
      "    print \"Parameters sets with scores:\"\n",
      "    i = 0\n",
      "    for p in params:\n",
      "        i+=1\n",
      "        print \"Parameters set\" , i\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"%s: %r\" % (param_name, p[0][param_name]))\n",
      "        print \"Score: \" , p[1]\n",
      "        print\n",
      "\n",
      "    y_predicted = gs_clf.predict(docs_test)\n",
      "\n",
      "    print(metrics.classification_report(y_test, y_predicted,\n",
      "                                        target_names=dataset.target_names))\n",
      "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
      "    print(cm)\n",
      "    plt.matshow(cm)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n_samples: 2000\n",
        "grid search"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parameters sets with scores:\n",
        "Parameters set 1\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.53\n",
        "\n",
        "Parameters set 2\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.535\n",
        "\n",
        "Parameters set 3\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.5575\n",
        "\n",
        "Parameters set 4\n",
        "clf__alpha: 0.01\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.54\n",
        "\n",
        "Parameters set 5\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.7225\n",
        "\n",
        "Parameters set 6\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.7525\n",
        "\n",
        "Parameters set 7\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.65\n",
        "\n",
        "Parameters set 8\n",
        "clf__alpha: 0.001\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.645\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.79      0.74      0.77       254\n",
        "        pos       0.75      0.80      0.77       246\n",
        "\n",
        "avg / total       0.77      0.77      0.77       500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[188  66]\n",
        " [ 49 197]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABVxJREFUeJzt2z+LXGUfx+HvzwTfhATSKGhnYxdIuZ2lpNTa3lJ9D7bG\nUm3t7JSAlZ2gghYL0cLKF6BwP0X2gRUyO0Nmds/69brgwJwzw80Plg/3mT87a60AXV7aegDg9IQN\nhYQNhYQNhYQNhYQNhYR9hZk5m5mfZ+aXmflg63k4zMw8npk/ZuaHrWfZirB3mJk7ST5JcpbkjSSP\nZub1bafiQJ/l2d/tP0vYu72V5Ne11vla668kXyR5e+OZOMBa60mSP7eeY0vC3u2VJE8vnf92cQ1u\nPWHv5re2/GsJe7ffk9y7dH4vz3ZtuPWEvdv3SV6dmfsz83KSd5J8tfFMcBBh77DW+jvJ+0m+TvJj\nki/XWj9tOxWHmJnPk3yX5LWZeToz7249000b/7YJfezYUEjYUEjYUEjYUEjYUOjusQvMjI/VYSNr\nrXne9aPDTpInp1jklnqc5L2th7hGD+p/OfvRxdHouU0ncSsOlYQNhYS9x5tbD8CRHm49wCaEvYew\n/+0ebj3AJoQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQN\nhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQN\nhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhYQNhfaGPTNnM/PzzPwyMx/cxFDAca4Me2buJPkkyVmS\nN5I8mpnXb2Iw4MXt27HfSvLrWut8rfVXki+SvH39YwHH2Bf2K0meXjr/7eIacIvd3fP8OmSRx5ce\nv3lxAKf2zcWx376wf09y79L5vTzbtf/hvcOmAo7y8OL4v493vnLfrfj3SV6dmfsz83KSd5J8deR0\nwDW7csdea/09M+8n+TrJnSSfrrV+upHJgBc2ax30Nnr3AjPryYmG4eY9OOxjFG6lyVprnveMX55B\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFD\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFD\nIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDIWFDobunWORBPjzFMmzgw8zWI/CCPr7i\nOTs2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2\nFBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2\nFBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FNob9sw8npk/ZuaHmxgION4h\nO/ZnSc6uexDgdPaGvdZ6kuTPG5gFOBHvsaHQ3dMs882lx/cvDuCUzi+OQ5wo7IenWQbY6X7+uWV+\ne8Vr3YpDoUO+7vo8yXdJXpuZpzPz7vWPBRxj7634WuvRTQwCnI5bcSgkbCgkbCgkbCgkbCgkbCgk\nbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgk\nbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgk\nbCgkbCgkbCgk7L3Otx6AI5xvPcBGhL3X+dYDcITzrQfYiLChkLCh0Ky1jltg5rgFgBe21prnXT86\nbOD2cSsOhYQNhYQNhYQNhYQNhf4HOgGIKhf9mdMAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fb7982b2590>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Exercise 2  Decision Tree\n",
      "from sklearn import tree\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    movie_reviews_data_folder = './movies_opinions/'\n",
      "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
      "    print(\"n_samples: %d\" % len(dataset.data))\n",
      "\n",
      "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
      "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
      "     \n",
      "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', tree.DecisionTreeClassifier()),])\n",
      "    text_clf = text_clf.fit(docs_train[:limit], y_train[:limit])\n",
      "\n",
      "    \n",
      "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__min_samples_split': (2, 4),\n",
      "              'clf__max_leaf_nodes': (None, 2),}\n",
      "    \n",
      "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)    \n",
      "    gs_clf = gs_clf.fit(docs_train[:limit], y_train[:limit])\n",
      "    params = gs_clf.grid_scores_\n",
      "    \n",
      "    print \"Parameters sets with scores:\"\n",
      "    i = 0\n",
      "    for p in params:\n",
      "        i+=1\n",
      "        print \"Parameters set\" , i\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"%s: %r\" % (param_name, p[0][param_name]))\n",
      "        print \"Score: \" , p[1]\n",
      "        print\n",
      "    \n",
      "    y_predicted = gs_clf.predict(docs_test)\n",
      "\n",
      "    print(metrics.classification_report(y_test, y_predicted,\n",
      "                                        target_names=dataset.target_names))\n",
      "\n",
      "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
      "    print(cm)\n",
      "    plt.matshow(cm)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n_samples: 2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parameters sets with scores:\n",
        "Parameters set 1\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.6225\n",
        "\n",
        "Parameters set 2\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.635\n",
        "\n",
        "Parameters set 3\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.605\n",
        "\n",
        "Parameters set 4\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.5975\n",
        "\n",
        "Parameters set 5\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.605\n",
        "\n",
        "Parameters set 6\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.6075\n",
        "\n",
        "Parameters set 7\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.5675\n",
        "\n",
        "Parameters set 8\n",
        "clf__max_leaf_nodes: None\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.6125\n",
        "\n",
        "Parameters set 9\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.5775\n",
        "\n",
        "Parameters set 10\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.5875\n",
        "\n",
        "Parameters set 11\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.56\n",
        "\n",
        "Parameters set 12\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 2\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.575\n",
        "\n",
        "Parameters set 13\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.5775\n",
        "\n",
        "Parameters set 14\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.5875\n",
        "\n",
        "Parameters set 15\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.56\n",
        "\n",
        "Parameters set 16\n",
        "clf__max_leaf_nodes: 2\n",
        "clf__min_samples_split: 4\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.575\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.58      0.63      0.60       242\n",
        "        pos       0.62      0.57      0.60       258\n",
        "\n",
        "avg / total       0.60      0.60      0.60       500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[152  90]\n",
        " [110 148]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXtJREFUeJzt2zGLZGUahuHn3Rn8EzLQiYLCJrsgE07YmaFMaG5uuOt/\nMFtWQ93UDcRsFkEmcCYRVkGDgdHAyB+g8BnMgC10TRVd3X3ax+uCgjqniq9fONx8p7qrZ60VoMtf\nth4AuHzChkLChkLChkLChkLChkLCfoGZOZ2Zb2bm25l5d+t5OMzMfDAzP87MV1vPshVh7zAzt5K8\nn+Q0yetJ7s/Ma9tOxYE+zLPr9qcl7N3eSPLdWuvJWuvnJB8neXPjmTjAWuvzJD9tPceWhL3by0me\nnjn+/vk5uPGEvZvv2vKHJezdfkhy58zxnTzbteHGE/ZuXyZ5ZWZOZualJG8l+WTjmeAgwt5hrfVL\nkneSfJbk/0n+s9b6etupOMTMfJTkiySvzszTmXl765mu2/i3Tehjx4ZCwoZCwoZCwoZCwoZCt49d\nYGb8Wh02staa884fHXaS/OMyFrmhHiS5t/EMV+m96quXdF/B93a+4lYcCgkbCgl7j5OtB+BIJ1sP\nsAlh73Gy9QAc6WTrATYhbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgk\nbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgk\nbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCgkbCi0N+yZOZ2Zb2bm25l59zqGAo7zwrBn\n5laS95OcJnk9yf2Zee06BgMubt+O/UaS79ZaT9ZaPyf5OMmbVz8WcIx9Yb+c5OmZ4++fnwNusNt7\nXl+HLPLgzPOT5w/gsj15/thvX9g/JLlz5vhOnu3av3PvoB8FHOckv982/7fznftuxb9M8srMnMzM\nS0neSvLJkdMBV+yFO/Za65eZeSfJZ0luJfn3Wuvra5kMuLB9t+JZa32a5NNrmAW4JL55BoWEDYWE\nDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWE\nDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWE\nDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYWEDYVmrXXcAjMrD49bg+08vDtbj8AF3U2y1jr3\nAtqxoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCw\noZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCw\noZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwoZCwodDesGfmg5n5cWa+uo6BgOMd\nsmN/mOT0qgcBLs/esNdanyf56RpmAS6Jz9hQ6PalrPKvf/72/G/3kr/fu5Rlgd88SvL4wPfOWmv/\nm2ZOkvx3rfXXc15bebh/DW6mh3dn6xG4oLtJ1lrnXkC34lDokD93fZTkiySvzszTmXn76scCjrH3\nM/Za6/51DAJcHrfiUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjY\nUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjY\nUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjYUEjY+zx6sPUEHOHR1gNsRNj7PH6w\n9QQc4fHWA2xE2FBI2FBo1lrHLTBz3ALAha215rzzR4cN3DxuxaGQsKGQsKGQsKGQsKHQr02bml5T\nxPkGAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f4bc30d4390>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Exercise 2 kNN\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    movie_reviews_data_folder = './movies_opinions/'\n",
      "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
      "    print(\"n_samples: %d\" % len(dataset.data))\n",
      "\n",
      "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
      "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
      "     \n",
      "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', KNeighborsClassifier(n_neighbors=3)),])\n",
      "    text_clf = text_clf.fit(docs_train[:400], y_train[:400])\n",
      "\n",
      "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__n_neighbors': (3, 5),}\n",
      "    \n",
      "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
      "    \n",
      "    gs_clf = gs_clf.fit(docs_train, y_train)\n",
      "    \n",
      "    params = gs_clf.grid_scores_\n",
      "        \n",
      "    print \"Parameters sets with scores:\"\n",
      "    i = 0\n",
      "    for p in params:\n",
      "        i+=1\n",
      "        print \"Parameters set\" , i\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"%s: %r\" % (param_name, p[0][param_name]))\n",
      "        print \"Score: \" , p[1]\n",
      "        print\n",
      "   \n",
      "    y_predicted = gs_clf.predict(docs_test)\n",
      "\n",
      "    \n",
      "    print(metrics.classification_report(y_test, y_predicted,\n",
      "                                        target_names=dataset.target_names))\n",
      "\n",
      "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
      "    print(cm)\n",
      "    plt.matshow(cm)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "n_samples: 2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parameters sets with scores:\n",
        "Parameters set 1\n",
        "clf__n_neighbors: 3\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.591333333333\n",
        "\n",
        "Parameters set 2\n",
        "clf__n_neighbors: 3\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.576\n",
        "\n",
        "Parameters set 3\n",
        "clf__n_neighbors: 3\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.576\n",
        "\n",
        "Parameters set 4\n",
        "clf__n_neighbors: 3\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.554666666667\n",
        "\n",
        "Parameters set 5\n",
        "clf__n_neighbors: 5\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.566\n",
        "\n",
        "Parameters set 6\n",
        "clf__n_neighbors: 5\n",
        "tfidf__use_idf: True\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.559333333333\n",
        "\n",
        "Parameters set 7\n",
        "clf__n_neighbors: 5\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 1)\n",
        "Score:  0.574\n",
        "\n",
        "Parameters set 8\n",
        "clf__n_neighbors: 5\n",
        "tfidf__use_idf: False\n",
        "vect__ngram_range: (1, 2)\n",
        "Score:  0.541333333333\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.74      0.41      0.53       235\n",
        "        pos       0.63      0.88      0.73       265\n",
        "\n",
        "avg / total       0.68      0.66      0.63       500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 96 139]\n",
        " [ 33 232]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXtJREFUeJzt27FrnHUcx/Hv1wb/hoIEshiwk+3QRYeMGYSOpZvSTdy7\nxv9B3Kp0qq4iiltFcXJQCra0HQJpoZ36B7Twc2jQFHq5I3e5J358veAgz93x8IHw5nmSXHqMUUCW\nt6YeAKyesCGQsCGQsCGQsCGQsCGQsI/R3bvdfb+7H3b3jan3sJju/qq7n3X33am3TEXYM3T3uar6\noqp2q+pCVV3r7vemXcWCvq5X37f/LWHPdrmqHo0x9scYL6rqm6q6MvEmFjDG+KWqnk+9Y0rCnu2d\nqjo4cvz48Dk484Q9m8/a8p8l7NmeVNXmkePNenXVhjNP2LP9XlXvdvdWd79dVVer6ruJN8FChD3D\nGONlVX1WVT9V1V9V9e0Y4960q1hEd9+uqt+qaru7D7r7k6k3rVv7t03I44oNgYQNgYQNgYQNgYQN\ngTaWPUF3+7U6TGSM0W96fumwq6rq1+C2b+5VXd+besWp+fKDj6eecKq+3/ujPtp7f+oZp+LTvjXz\nNbfiEEjYEEjY81zcmXoBS9jeOT/1hEkIe55LO1MvYAnCBmIIGwIJGwIJGwIJGwIJGwIJGwIJGwIJ\nGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJ\nGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwLNDbu7\nd7v7fnc/7O4b6xgFLOfYsLv7XFV9UVW7VXWhqq5193vrGAac3Lwr9uWqejTG2B9jvKiqb6rqyunP\nApYxL+x3qurgyPHjw+eAM2xjzutjobPc3Pv364s7VZd2TjgHmOXBnaf14M7Thd47L+wnVbV55Hiz\nXl21X3d9b8FpwElt75yv7Z3z/xz/8PmfM98771b896p6t7u3uvvtqrpaVd+tYiRweo69Yo8xXnb3\nZ1X1U1Wdq6qbY4x7a1kGnNi8W/EaY/xYVT+uYQuwIj55BoGEDYGEDYGEDYGEDYGEDYGEDYGEDYGE\nDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGE\nDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGEDYGE\nDYGEDYGEDYGEDYGEDYE2VnKWD/dWchrW71ndmnoCp8AVGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJ\nGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJ\nGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJGwIJ\nGwIJGwIJGwIJGwIJGwLNDbu7v+ruZ919dx2DgOUtcsX+uqp2T3sIsDpzwx5j/FJVz9ewBVgRP2ND\noI3VnObOka+3Dh/AKu0fPhaxorB3VnMaYKatev2S+fMx73UrDoEW+XPX7ar6raq2u/uguz85/VnA\nMubeio8xrq1jCLA6bsUhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAh\nkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAh\nkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLAhkLDn2p96AEvYn3rARIQ91/7U\nA1jC/tQDJiJsCCRsCNRjjOVO0L3cCYATG2P0m55fOmzg7HErDoGEDYGEDYGEDYGEDYH+BuchkSlW\nuX4JAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f4b892e45d0>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Exercise 2 Neural network\n",
      "\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    movie_reviews_data_folder = './movies_opinions/'\n",
      "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
      "    print(\"n_samples: %d\" % len(dataset.data))\n",
      "\n",
      "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
      "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
      "     \n",
      "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)),])\n",
      "    text_clf = text_clf.fit(docs_train[:400], y_train[:400])\n",
      "    \n",
      "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
      "              'tfidf__use_idf': (True, False),\n",
      "              'clf__alpha': (1e-3, 1e-5),}\n",
      "    \n",
      "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
      "    \n",
      "    gs_clf = gs_clf.fit(docs_train[:400], y_train[:400])\n",
      "    \n",
      "    params = gs_clf.grid_scores_\n",
      "        \n",
      "    print \"Parameters sets with scores:\"\n",
      "    i = 0\n",
      "    for p in params:\n",
      "        i+=1\n",
      "        print \"Parameters set\" , i\n",
      "        for param_name in sorted(parameters.keys()):\n",
      "            print(\"%s: %r\" % (param_name, p[0][param_name]))\n",
      "        print \"Score: \" , p[1]\n",
      "        print\n",
      "    \n",
      "    y_predicted = gs_clf.predict(docs_test)\n",
      "\n",
      "    print(metrics.classification_report(y_test, y_predicted,\n",
      "                                        target_names=dataset.target_names))\n",
      "\n",
      "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
      "    print(cm)\n",
      "    plt.matshow(cm)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name MLPClassifier",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-713738dd9d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Exercise 2 Neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: cannot import name MLPClassifier"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}